import streamlit as st
from threading import Thread
import torch
from transformers import TextIteratorStreamer, AutoTokenizer

# è‡ªå®šä¹‰æ¨¡å‹å¯¼å…¥
from model.model_big_strong import BigStrongForCausalLM, BigStrongConfig

# RAG ç›¸å…³åº“
import os
import fitz
from docx import Document
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import json
import hashlib
from datetime import datetime

# å…¨å±€å˜é‡
MODEL_PATH = "../model/full_sft_512.pth"
TOKENIZER_PATH = "../model"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DOCUMENT_FOLDER = "../doc"  # æ–‡æ¡£è·¯å¾„
VECTOR_DB_FOLDER = "../vector_db"  # å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„


@st.cache_resource
def load_model():
    status_text = st.empty()
    status_text.text("æ­£åœ¨åŠ è½½è¯­è¨€æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)
    # å¯¹é½åˆ†è¯å™¨ä¸æ¨¡å‹çš„é‡è¦æ ‡è¯†ç¬¦ï¼Œé¿å…ç”Ÿæˆç«‹å³ç»“æŸ
    bos_id = getattr(tokenizer, "bos_token_id", None)
    eos_id = getattr(tokenizer, "eos_token_id", None)
    pad_id = getattr(tokenizer, "pad_token_id", None)
    vocab_size = getattr(tokenizer, "vocab_size", None) or len(tokenizer)

    config = BigStrongConfig(
        hidden_size=512,
        num_hidden_layers=8,
        bos_token_id=bos_id if bos_id is not None else 1,
        eos_token_id=eos_id if eos_id is not None else 2,
        vocab_size=vocab_size,
    )
    model = BigStrongForCausalLM(config)

    state_dict = torch.load(MODEL_PATH, map_location="cpu")
    model.load_state_dict(state_dict, strict=False)
    model = model.to(DEVICE).eval()
    # è®¾ç½®ç”Ÿæˆé…ç½®ï¼Œç¡®ä¿ä¸ tokenizer å¯¹é½
    try:
        model.generation_config.pad_token_id = (
            pad_id if pad_id is not None else config.eos_token_id
        )
        model.generation_config.bos_token_id = config.bos_token_id
        model.generation_config.eos_token_id = config.eos_token_id
    except Exception:
        pass

    status_text.text("è¯­è¨€æ¨¡å‹åŠ è½½å®Œæˆï¼")
    status_text.empty()
    return model, tokenizer


def get_documents_hash(document_folder):
    """è·å–æ–‡ä»¶åå’Œä¿®æ”¹æ—¶é—´çš„å“ˆå¸Œå€¼æ¥åˆ¤æ–­æ–‡æ¡£æ˜¯å¦æœ‰å˜æ›´"""
    hash_md5 = hashlib.md5()

    if not os.path.exists(document_folder):
        return ""

    # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    supported_exts = {".txt", ".pdf", ".docx"}
    files_info = []

    for file_name in sorted(os.listdir(document_folder)):
        file_path = os.path.join(document_folder, file_name)
        if os.path.isfile(file_path):
            ext = os.path.splitext(file_name)[1].lower()
            if ext in supported_exts:
                # è®°å½•æ–‡ä»¶åå’Œä¿®æ”¹æ—¶é—´
                mtime = os.path.getmtime(file_path)
                files_info.append(f"{file_name}:{mtime}")

    # è®¡ç®—æ•´ä½“å“ˆå¸Œ
    files_str = "|".join(files_info)
    hash_md5.update(files_str.encode("utf-8"))
    return hash_md5.hexdigest()


def save_vector_db(documents, embedding_model_name, index, vector_db_folder, docs_hash):
    """ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç£ç›˜"""
    os.makedirs(vector_db_folder, exist_ok=True)

    # ä¿å­˜æ–‡æ¡£åˆ—è¡¨
    with open(os.path.join(vector_db_folder, "documents.pkl"), "wb") as f:
        pickle.dump(documents, f)

    # ä¿å­˜ FAISS ç´¢å¼•
    faiss.write_index(index, os.path.join(vector_db_folder, "faiss_index.bin"))

    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "embedding_model": embedding_model_name,
        "documents_count": len(documents),
        "docs_hash": docs_hash,
        "created_time": datetime.now().isoformat(),
        "dimension": index.d,  # å‘é‡ç»´åº¦
    }

    with open(
        os.path.join(vector_db_folder, "metadata.json"), "w", encoding="utf-8"
    ) as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    st.success(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°ï¼š{vector_db_folder}")


# --- åŠ è½½å‘é‡æ•°æ®åº“ ---
def load_vector_db(vector_db_folder, embedding_model_name):
    """ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“"""
    try:
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ["documents.pkl", "faiss_index.bin", "metadata.json"]
        for file_name in required_files:
            if not os.path.exists(os.path.join(vector_db_folder, file_name)):
                return None, None, None

        # åŠ è½½å…ƒæ•°æ®
        with open(
            os.path.join(vector_db_folder, "metadata.json"), "r", encoding="utf-8"
        ) as f:
            metadata = json.load(f)

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŒ¹é…
        if metadata.get("embedding_model") != embedding_model_name:
            st.warning(
                f"å‘é‡æ•°æ®åº“ä½¿ç”¨çš„æ¨¡å‹ ({metadata.get('embedding_model')}) ä¸å½“å‰æ¨¡å‹ ({embedding_model_name}) ä¸åŒ¹é…ï¼Œå°†é‡æ–°æ„å»º"
            )
            return None, None, None

        # åŠ è½½æ–‡æ¡£åˆ—è¡¨
        with open(os.path.join(vector_db_folder, "documents.pkl"), "rb") as f:
            documents = pickle.load(f)

        # åŠ è½½ FAISS ç´¢å¼•
        index = faiss.read_index(os.path.join(vector_db_folder, "faiss_index.bin"))

        st.info(
            f"æˆåŠŸåŠ è½½å‘é‡æ•°æ®åº“ï¼š{len(documents)} ä¸ªæ–‡æ¡£å—ï¼Œåˆ›å»ºæ—¶é—´ï¼š{metadata.get('created_time')}"
        )
        return documents, index, metadata.get("docs_hash")

    except Exception as e:
        st.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼š{e}")
        return None, None, None


@st.cache_resource
def initialize_rag(
    document_folder=DOCUMENT_FOLDER,
    vector_db_folder=VECTOR_DB_FOLDER,
    force_rebuild=False,
):
    # ä¸­æ–‡ embedding æ¨¡å‹ï¼ˆåç§°å®é™…æ˜¯è‹±æ–‡æ¨¡å‹ï¼Œä½†é€šç”¨æ€§å¼ºï¼‰
    embedding_model_name = "all-MiniLM-L6-v2"
    embedding_model = SentenceTransformer(embedding_model_name)

    # è®¡ç®—å½“å‰æ–‡æ¡£çš„å“ˆå¸Œå€¼
    current_docs_hash = get_documents_hash(document_folder)

    # å¦‚æœä¸å¼ºåˆ¶é‡å»ºï¼Œå°è¯•åŠ è½½å·²ä¿å­˜çš„å‘é‡åº“
    if not force_rebuild:
        documents, index, saved_docs_hash = load_vector_db(
            vector_db_folder, embedding_model_name
        )
        # å¦‚æœå‘é‡åº“å­˜åœ¨ä¸”æ–‡æ¡£æœªå˜åŒ–ï¼Œç›´æ¥å¤ç”¨
        if documents is not None and saved_docs_hash == current_docs_hash:
            st.success("âœ… ä½¿ç”¨ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£æ— å˜æ›´")
            return documents, embedding_model, index
        elif documents is not None and saved_docs_hash != current_docs_hash:
            st.info("ğŸ“ æ£€æµ‹åˆ°æ–‡æ¡£å†…å®¹å˜åŒ–ï¼Œé‡æ–°æ„å»ºå‘é‡æ•°æ®åº“...")
        else:
            st.info("ğŸ“ æœªæ‰¾åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œå¼€å§‹æ„å»º...")
    else:
        st.warning("ğŸ”§ å¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“ï¼ˆforce_rebuild=Trueï¼‰")

    # ========== æ„å»ºæ–°çš„å‘é‡æ•°æ®åº“ ==========
    st.write("ğŸ”¨ å¼€å§‹æ„å»ºæ–°çš„å‘é‡æ•°æ®åº“...")

    documents = []
    supported_exts = {".txt", ".pdf", ".docx"}

    # åŠ è½½supported_extsç±»çš„æ–‡æ¡£
    files_to_process = [
        f
        for f in os.listdir(document_folder)
        if os.path.isfile(os.path.join(document_folder, f))
        and os.path.splitext(f)[1].lower() in supported_exts
    ]

    if len(files_to_process) == 0:
        st.warning("âš ï¸ æœªåœ¨æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£ï¼ˆ.txt/.pdf/.docxï¼‰")
        return None, None, None

    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, file_name in enumerate(files_to_process):
        file_path = os.path.join(document_folder, file_name)
        status_text.text(f"æ­£åœ¨å¤„ç†: {file_name}")

        text = load_document(file_path)
        if text.strip():  # åªå¤„ç†éç©ºæ–‡æ¡£
            chunks = split_text(text, chunk_size=256, overlap=32)
            # ä¸ºæ¯ä¸ªå—æ·»åŠ æ¥æºä¿¡æ¯
            chunks_with_source = [f"[æ¥æº: {file_name}]\n{chunk}" for chunk in chunks]
            documents.extend(chunks_with_source)

        progress_bar.progress((i + 1) / len(files_to_process))

    progress_bar.empty()
    status_text.empty()

    if len(documents) == 0:
        st.warning("âš ï¸ æ‰€æœ‰æ–‡æ¡£å‡ä¸ºç©ºæˆ–æ— æ³•è¯»å–ï¼Œæ— æ³•æ„å»ºå‘é‡æ•°æ®åº“")
        return None, None, None

    st.write(
        f"ğŸ“Š å…±å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æœ¬å—"
    )

    # ç”Ÿæˆå‘é‡å¹¶æ„å»º FAISS ç´¢å¼•
    with st.spinner("ğŸ§® æ­£åœ¨ç”Ÿæˆæ–‡æœ¬å‘é‡..."):
        embeddings = embedding_model.encode(
            documents, convert_to_numpy=True, show_progress_bar=True
        )

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)

    # ä¿å­˜å‘é‡åº“ï¼ˆå«å½“å‰æ–‡æ¡£å“ˆå¸Œï¼‰
    save_vector_db(
        documents, embedding_model_name, index, vector_db_folder, current_docs_hash
    )

    st.success("âœ… RAG å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    return documents, embedding_model, index


def load_document(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    text = ""
    try:
        if ext == ".txt":
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
        elif ext == ".pdf":
            doc = fitz.open(file_path)
            for page in doc:
                text += page.get_text()
            doc.close()
        elif ext == ".docx":
            doc = Document(file_path)
            text = "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{file_path}ï¼Œé”™è¯¯ï¼š{e}")
    return text


def split_text(text, chunk_size=256, overlap=32):
    """ç›¸é‚»å—ä¹‹é—´æœ‰32ä¸ªè¯çš„é‡å ï¼Œé¿å…å…³é”®ä¿¡æ¯è¢«åˆ†å‰²"""
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if start >= len(words):
            break
    return chunks


def retrieve_relevant_context(query, documents, embedding_model, index, top_k=3):
    query_vec = embedding_model.encode([query], convert_to_numpy=True)  # å°†æŸ¥è¯¢è½¬ä¸ºå‘é‡
    scores, indices = index.search(query_vec, top_k)

    context_parts = []
    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
        # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°ä¿¡æ¯
        similarity = 1 / (1 + score)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        context_parts.append(
            f"[å‚è€ƒèµ„æ–™ {i + 1}] (ç›¸ä¼¼åº¦: {similarity:.3f})\n{documents[idx]}"
        )

    return "\n\n".join(context_parts)


def get_model_response(
    prompt, max_new_tokens=512, temperature=0.85, top_p=0.8, use_rag=True
):
    history_messages = st.session_state.messages
    if use_rag:
        search_query = prompt
        if len(st.session_state.messages) >= 2:
            recent_context = []
            for msg in st.session_state.messages[-4:]:
                recent_context.append(f"{msg['role']}: {msg['content']}")
            search_query = (
                f"å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{' | '.join(recent_context)}\nå½“å‰é—®é¢˜ï¼š{prompt}"
            )

        context = retrieve_relevant_context(
            search_query, docs, emb_model, faiss_index, top_k=2
        )
        rag_messages = {
            "role": "user",
            "content": f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·å‚è€ƒä»¥ä¸‹èµ„æ–™å›ç­”é—®é¢˜ã€‚è‹¥èµ„æ–™ä¸­æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜"æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”"ã€‚\nã€å‚è€ƒèµ„æ–™ã€‘\n{context}\nè¯·åŸºäºä»¥ä¸Šèµ„æ–™å’Œå¯¹è¯å†å²ï¼Œè‡ªç„¶åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜{prompt}ã€‚""",
        }
        final_messages = history_messages + [rag_messages]
        final_prompt = tokenizer.apply_chat_template(
            final_messages, tokenize=False, add_generation_prompt=True
        )
    else:
        # éRAGæ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨å¯¹è¯å†å²
        final_messages = history_messages + [{"role": "user", "content": prompt}]
        final_prompt = tokenizer.apply_chat_template(
            final_messages, tokenize=False, add_generation_prompt=True
        )

    inputs = tokenizer(
        final_prompt, return_tensors="pt", truncation=True, max_length=2048
    ).to(DEVICE)
    streamer = TextIteratorStreamer(
        tokenizer, skip_prompt=True, skip_special_tokens=True
    )

    pad_id = (
        getattr(model.generation_config, "pad_token_id", None)
        or getattr(tokenizer, "pad_token_id", None)
        or getattr(model.generation_config, "eos_token_id", None)
    )
    eos_id = getattr(model.generation_config, "eos_token_id", None) or getattr(
        tokenizer, "eos_token_id", None
    )
    generation_kwargs = {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"],
        "max_new_tokens": max_new_tokens,
        "min_new_tokens": 8,
        "do_sample": True,
        "temperature": temperature,
        "top_p": top_p,
        "pad_token_id": pad_id,
        "eos_token_id": eos_id,
        "no_repeat_ngram_size": 3,
        "streamer": streamer,
    }
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    for text in streamer:
        # é€æ­¥å°†ç”Ÿæˆå†…å®¹è¿”å›ç»™è°ƒç”¨æ–¹ï¼ˆç”¨äº st.write_stream å®æ—¶æ˜¾ç¤ºï¼‰
        yield text


# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="BigStrongGPT-RAG",
    page_icon="ğŸ§ ",
    layout="centered",
    initial_sidebar_state="auto",
)

st.title("ğŸ§  BigStrongGPT + RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
st.markdown("åŸºäºæœ¬åœ°æ–‡æ¡£è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæ”¯æŒ `.txt`, `.pdf`, `.docx` æ–‡ä»¶")

# --- ä¾§è¾¹æ ï¼šRAG æ§åˆ¶ ---
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    use_rag = st.checkbox(
        "å¯ç”¨ RAG æ£€ç´¢å¢å¼º", value=True, help="å…³é—­åˆ™ä½¿ç”¨åŸå§‹æ¨¡å‹å›ç­”"
    )
    temperature = st.slider("Temperature", 0.1, 1.5, 0.85, 0.05)
    top_p = st.slider("Top-p", 0.1, 1.0, 0.8, 0.05)
    max_new_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 128, 1024, 512, 64)

    st.divider()
    st.header("ğŸ“š å‘é‡æ•°æ®åº“ç®¡ç†")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ”„ é‡æ–°æ„å»º", help="å¼ºåˆ¶é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“"):
            st.cache_resource.clear()
            initialize_rag(force_rebuild=True)
            st.success("å‘é‡æ•°æ®åº“å·²é‡æ–°æ„å»ºï¼")

    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç†ç¼“å­˜", help="æ¸…ç†æ‰€æœ‰ç¼“å­˜"):
            st.cache_resource.clear()
            st.success("ç¼“å­˜å·²æ¸…ç†ï¼")

    # æ˜¾ç¤ºå‘é‡æ•°æ®åº“çŠ¶æ€
    if os.path.exists(VECTOR_DB_FOLDER) and os.path.exists(
        os.path.join(VECTOR_DB_FOLDER, "metadata.json")
    ):
        try:
            with open(
                os.path.join(VECTOR_DB_FOLDER, "metadata.json"), "r", encoding="utf-8"
            ) as f:
                metadata = json.load(f)

            st.info(
                f"""
            ğŸ“Š **å‘é‡æ•°æ®åº“çŠ¶æ€**
            - æ–‡æ¡£å—æ•°é‡: {metadata.get('documents_count', 'N/A')}
            - å‘é‡ç»´åº¦: {metadata.get('dimension', 'N/A')}  
            - åˆ›å»ºæ—¶é—´: {metadata.get('created_time', 'N/A')[:19]}
            """
            )
        except Exception as e:
            raise e


model, tokenizer = load_model()
docs, emb_model, faiss_index = initialize_rag(force_rebuild=False)

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- å¤„ç†ç”¨æˆ·è¾“å…¥ ---
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)

    # æ˜¾ç¤ºåŠ©æ‰‹å›å¤
    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  ç”Ÿæˆä¸­..."):
            response = st.write_stream(
                get_model_response(
                    prompt,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    use_rag=use_rag,
                )
            )

    # ä¿å­˜å¯¹è¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.messages.append({"role": "assistant", "content": response})
